version: '3.8'

services:
  mysql_source:
    image: mysql:8.0
    container_name: mysql_source
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: admin123
      MYSQL_DATABASE: ipl_daily_db
      MYSQL_USER: ipl_user
      MYSQL_PASSWORD: ipl_password
    volumes:
      - mysql_data:/var/lib/mysql
      - ./mysql_init:/docker-entrypoint-initdb.d
      - ./data/ipl:/var/lib/mysql-files
    ports:
      - "3307:3306"
    networks:
      - ipl_network

  postgres_dw:
    image: postgres:15
    container_name: postgres_dw
    restart: unless-stopped
    environment:
      POSTGRES_USER: ipl_data
      POSTGRES_PASSWORD: ipl_data
      POSTGRES_DB: ipl_warehouse
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres_init:/docker-entrypoint-initdb.d
    ports:
      - "5433:5432"
    networks:
      - ipl_network

  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: custom-spark:3.5
    container_name: spark-master
    user: "1001:0"
    environment:
      - SPARK_MODE=master
    ports:
      - '8085:8080'
      - '7077:7077'
    volumes:
      - ./spark-entrypoint.sh:/usr/local/bin/spark-entrypoint.sh
      - ./pyspark_apps:/opt/spark-apps
      - ./data_lake:/opt/data_lake
      - ./pyspark_apps/jars:/opt/spark-jars
    entrypoint: ["/usr/local/bin/spark-entrypoint.sh"]
    command: ["spark-class", "org.apache.spark.deploy.master.Master"]
    networks:
      - ipl_network

  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: custom-spark:3.5
    container_name: spark-worker
    user: "1001:0"
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    volumes:
      - ./spark-entrypoint.sh:/usr/local/bin/spark-entrypoint.sh
      - ./pyspark_apps:/opt/spark-apps
      - ./data_lake:/opt/data_lake
      - ./pyspark_apps/jars:/opt/spark-jars
    entrypoint: ["/usr/local/bin/spark-entrypoint.sh"]
    command: ["spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    networks:
      - ipl_network

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: custom-airflow:2.8.0
    container_name: airflow-init
    depends_on:
      - postgres_dw
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://ipl_data:ipl_data@postgres_dw:5432/ipl_warehouse
      - POSTHOG_API_KEY=myposthogapikey
      - POSTHOG_HOST=https://app.posthog.com
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./airflow_logs:/opt/airflow/logs
      - ./airflow_plugins:/opt/airflow/plugins
    command: >
      bash -c "airflow db upgrade &&
      airflow users create --username ninad --firstname Ninad --lastname Badekar --role Admin --email ninad@example.com --password ninad@123 &&
      python /opt/airflow/plugins/init_connections.py"
    networks:
      - ipl_network
      

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: custom-airflow:2.8.0
    container_name: airflow-webserver
    restart: unless-stopped
    depends_on:
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://ipl_data:ipl_data@postgres_dw:5432/ipl_warehouse
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./airflow_logs:/opt/airflow/logs
      - ./airflow_plugins:/opt/airflow/plugins
      - ./pyspark_apps:/opt/pyspark_apps
      - ./data_lake:/opt/data_lake
    ports:
      - "8086:8080"
    command: webserver
    networks:
      - ipl_network

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: custom-airflow:2.8.0
    container_name: airflow-scheduler
    restart: unless-stopped
    depends_on:
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://ipl_data:ipl_data@postgres_dw:5432/ipl_warehouse
      - POSTHOG_API_KEY=phc_your_real_key_here
      - POSTHOG_HOST=https://app.posthog.com
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./airflow_logs:/opt/airflow/logs
      - ./airflow_plugins:/opt/airflow/plugins
      - ./pyspark_apps:/opt/pyspark_apps
      - ./data_lake:/opt/data_lake
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    networks:
      - ipl_network
  superset:
    image: apache/superset:2.1.0
    container_name: superset
    environment:
      - SUPERSET_SECRET_KEY=supersecretkey
      - ADMIN_USERNAME=admin
      - ADMIN_FIRST_NAME=Admin
      - ADMIN_LAST_NAME=User
      - ADMIN_EMAIL=admin@example.com
      - ADMIN_PASSWORD=admin123
      - DATABASE_URL=postgresql+psycopg2://ipl_data:ipl_data@postgres_dw:5432/ipl_warehouse
    ports:
      - "8088:8088"
    volumes:
      - superset_home:/app/superset_home
    depends_on:
      - postgres_dw
    networks:
      - ipl_network
    command: >
      bash -c "
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin123 &&
        superset init &&
        superset run -h 0.0.0.0 -p 8088"

volumes:
  mysql_data:
  postgres_data:
  superset_home:

networks:
  ipl_network:
    driver: bridge
